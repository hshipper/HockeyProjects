---
title: "HockeyScraping"
author: "Henry Shipper"
date: "August 19, 2015"
output: html_document
---

This document is meant to demonstrate the process of scraping [Hockey-Reference.com](http://www.hockey-reference.com) to create tables of players using the **XML** and **RCurl** packages. These tables will later be used (and code from this document will be reused) in performing analysis on data from other tables taken from this website. 

### Installing and loading packages
The following packages are necessary:  
* plyr
* XML
* RCurl

**parallel** is optional, but will make running the code faster.

```{r, warning = FALSE, message = FALSE, results = 'hide', cache = TRUE}
packages <- c("plyr", "XML", "RCurl", "parallel")
lapply(packages, require, character.only = TRUE)
```

### Scraping player name tables
[Hockey-Reference.com](http://www.hockey-refence.com) lists all players ever to play in the NHL on pages determined by the first letter of their last name. As a result, we will need to download tables for each letter. A quick look at the "players" landing page shows us that there are no players whose last name starts with the letter "X".  
The **XML** package provides a convenient way to get these tables. We need only give it the url of the page containing the table we want. Since we will want to get tables from multiple pages, it is useful to create a function for this.

```{r}
fetch_players <- function(letter){
  if(!exists("player_tables")) player_tables <- list()
  url <- paste0("http://www.hockey-reference.com", letter, "/skaters.html")
  player_tables[[letter]] <- readHTMLTable(url)$skaters
  player_tables
}
```

Unfortunately, since throughout hockey history some players have had the exact same name, name alone is not sufficient to identify these players in other tables. The solution to this is that each player is assigned a unique identifier code that is hyperlinked from their name on the page. Since this is not contained in the table, we need to take another pass at the HTML source and pull the identifiers and then attach them to the tables. To avoid making two passes of the site's source, we can store the text source using `getURI()` from **RCurl**, then pass that along to `readHTMLTable()` instead of using the url again. 


```{r}
fetch_players <- function(letter){
  player_tables <- list()
  url <- paste0("http://www.hockey-reference.com/players/", letter, "/skaters.html")
  text <- getURI(url)
  player_tables[[letter]] <- readHTMLTable(text)$skaters
  matches <- gregexpr(paste0("/players/[a-z]/[a-z]{5,7}[0-9]{2}.html"), text)
  ids <- regmatches(text, matches)
  player_tables[[letter]][["id"]] <- ids[[1]]
  player_tables[[letter]]
}
```

Since this can take a long time, it is useful to use parallelization. Two potential ways to return the list of player tables are provided. One uses `lapply()`, the other uses `mclapply()` from **parallel**. It is important to note the warning in the documentation from **parallel**: 

> It is *strongly discouraged* to use these functions in GUI or embedded environments, because it leads to several processes sharing the same GUI which will likely cause chaos (and possibly crashes). Child processes should never use on-screen graphics devices.  

Since this function does not produce any text, I felt safe using it in RStudio and did not encounter any issues. 

#### Using `lapply()`
```{r, eval = FALSE}
all_player_tables <- lapply(letters[-24], fetch_players)
```

#### Using `mclapply()`
```{r, cache = TRUE}
all_player_tables <- mclapply(letters[-24], fetch_players)
```

##### Difference in speed
```{r, cache = TRUE}
mcl <- function() mclapply(letters[-24], fetch_players)
lap <- function() lapply(letters[-24], fetch_players)
system.time(mcl())
system.time(lap())
```

### Storing the tables
